# -*- coding: utf-8 -*-
"""NVDDataFetch-V4-API-Public.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C7jjJ9BM51oirxAyXpvWkZKPMBBx_q0C

# Setup our vulnerability data science lab environment

First we'll import all the libraries we need. A couple of them need installed first. JQ is a pythonic implementation of jq; a tool for querying json really fast. When looking at 25 years of vulnerabilities it is enormously useful.
"""

# Let's run the imports
import requests
import pandas as pd
from tqdm import tqdm
import os
import jq
import numpy as np
import re
from pandas.plotting import autocorrelation_plot
import time
import requests
import os
from datetime import datetime
import json

"""The folders where we will store the data as json files need to be created. We store 2000 vulnerabilities per file for simpliciity. They are indexed according to their numbers."""

file_exists = os.path.exists('CVE-NVD')
if not file_exists:
  os.mkdir('CVE-NVD')
  os.mkdir('CVE-NVD/JSON')

"""Now we'll download all the NVD data since 1999 using their API. You will need to provide your own API key, which you can get from NVD itself.
PROTIP: The progress bar comes for free from the tqdm package. Just wrap a for loop in tqdm.tqdm() it and you get a progress bar for free. Now after this tutorial if you keep this notebook, you'll always be able to fetch all this CVE data easily. Handy for many more things than just forecasting.
NOTE: We need to merge these data structures, as we may have updates for some vulnerabilities.
"""

import time
import requests
import os
import tqdm

# Placeholder for your API key from NVD
API_KEY = ""

# Base URL for the NVD API
BASE_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"

# Create directories if they don't exist
file_exists = os.path.exists('CVE-NVD')
if not file_exists:
    os.mkdir('CVE-NVD')
    os.mkdir('CVE-NVD/JSON')

# Rate limit: 50 requests per 30 seconds
RATE_LIMIT = 50
RATE_LIMIT_WINDOW = 30  # seconds

# Counter for requests
request_count = 0
start_time = time.time()

# Pagination parameters
start_index = 0
results_per_page = 2000  # Maximum allowed by the API

while True:
    params = {
        "startIndex": start_index,
        "resultsPerPage": results_per_page,
    }
    headers = {'apiKey': API_KEY}

    response = requests.get(BASE_URL, params=params, headers=headers)

    # Rate limiting logic
    request_count += 1
    if request_count >= RATE_LIMIT:
        elapsed_time = time.time() - start_time
        if elapsed_time < RATE_LIMIT_WINDOW:
            time.sleep(RATE_LIMIT_WINDOW - elapsed_time)
        request_count = 0
        start_time = time.time()

    if response.status_code == 200:
        data = response.json()
        total_results = data.get("totalResults", 0)

        # Save the current page of results
        with open(f'CVE-NVD/JSON/cve_data_{start_index}.json', 'w') as f:
            f.write(response.text)

        # Check if we have fetched all results
        if start_index + results_per_page >= total_results:
            print("All data has been fetched succesfully.")
            break

        # Update the start index for the next page
        start_index += results_per_page
    elif response.status_code == 522:
        print('Network issues trying this request again.')
        response = requests.get(BASE_URL, params=params, headers=headers)
        if response.status_code == 200:
            data = response.json()
            total_results = data.get("totalResults", 0)

            # Save the current page of results
            with open(f'CVE-NVD/JSON/cve_data_{start_index}.json', 'w') as f:
                f.write(response.text)
        else:
            print("Two network failures in a row, quitting datafetch. Please re-run the code later.")
            break
    elif response.status_code == 401:
        print('Check your API key')
        break
    else:
        print(f"Failed to fetch data: {response.status_code}")
        break

"""# Convert the data to panda dataframes and csv files

Here we start to use JQ to make queiries specific to CVE json structure. We pull out the CVE-ID, the published date, the assigner, and the CVSSv2 base score, as well as other details and push them into a pandas dataframe. This dataframe can then be saved and reused regularly.
"""

# Combined jq query to extract all relevant vulnerability data
vuln_query = jq.compile("""
  .vulnerabilities[] | {
    "ID": .cve.id,
    "Publication": .cve.published,
    "ASSIGNER": .cve.sourceIdentifier,
    "DESCRIPTION": [.cve.descriptions[].value],
    "v2 CVSS": (if .cve.metrics.cvssMetricV2 and (.cve.metrics.cvssMetricV2 | length > 0)
                     then .cve.metrics.cvssMetricV2[0].cvssData.baseScore
                     else null end),
    "v2 Exploitability Score": (if .cve.metrics.cvssMetricV2 and (.cve.metrics.cvssMetricV2 | length > 0)
                                    then .cve.metrics.cvssMetricV2[0].exploitabilityScore
                                    else null end),
    "v2 Vector": (if .cve.metrics.cvssMetricV2 and (.cve.metrics.cvssMetricV2 | length > 0)
                     then .cve.metrics.cvssMetricV2[0].cvssData.vectorString
                     else null end),
    "v3 CVSS": (if .cve.metrics.cvssMetricV31 and (.cve.metrics.cvssMetricV31 | length > 0)
                     then .cve.metrics.cvssMetricV31[0].cvssData.baseScore
                     elif .cve.metrics.cvssMetricV30 and (.cve.metrics.cvssMetricV30 | length > 0)
                     then .cve.metrics.cvssMetricV30[0].cvssData.baseScore
                     else null end),
    "v3 Vector": (if .cve.metrics.cvssMetricV31 and (.cve.metrics.cvssMetricV31 | length > 0)
                     then .cve.metrics.cvssMetricV31[0].cvssData.vectorString
                     elif .cve.metrics.cvssMetricV30 and (.cve.metrics.cvssMetricV30 | length > 0)
                     then .cve.metrics.cvssMetricV30[0].cvssData.vectorString
                     else null end),
    "v3 Exploitability Score": (if .cve.metrics.cvssMetricV31 and (.cve.metrics.cvssMetricV31 | length > 0)
                                    then .cve.metrics.cvssMetricV31[0].exploitabilityScore
                                    elif .cve.metrics.cvssMetricV30 and (.cve.metrics.cvssMetricV30 | length > 0)
                                    then .cve.metrics.cvssMetricV30[0].exploitabilityScore
                                    else null end),
    "v2.3 CPE": [.cve.configurations[]?.nodes[].cpeMatch[]? | select(.vulnerable == true) | .criteria] // [],
    "CWE": [.cve.weaknesses[]?.description[].value],
    "VulnStatus": .cve.vulnStatus
  }
""")

# Function to process a single file and extract vulnerabilities
def process_file(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)  # Load the JSON data from the file

    # Apply the jq query to extract vulnerabilities
    vuln_data = vuln_query.input(data).all()  # List of dictionaries for each vulnerability

    return vuln_data

# Function to process multiple files in a directory with progress bar
def process_directory(directory_path):
    all_vulns = []  # List to hold vulnerabilities from all files
    json_files = [f for f in os.listdir(directory_path) if f.endswith('.json')]  # Filter JSON files

    # Use tqdm to create a progress bar for file processing
    for filename in tqdm.tqdm(json_files, desc="Processing Files", unit="file"):
        file_path = os.path.join(directory_path, filename)

        # Process each file
        vuln_data = process_file(file_path)
        all_vulns.extend(vuln_data)  # Append the extracted data from this file

    # Return a list of all vulnerabilities found
    return all_vulns

# Define the directory where your JSON files are stored
json_dir = 'CVE-NVD/JSON/'

# Process all JSON files in the directory
vulnerabilities = process_directory(json_dir)

# Convert the list of dictionaries to a pandas DataFrame
df = pd.DataFrame(vulnerabilities)

# Optional: Clean up list-based fields (like 'description', 'cpe_criteria', 'cwe')
df['DESCRIPTION'] = df['DESCRIPTION'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')
#df['v2.3 CPE'] = df['v2.3 CPE'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')
df['CWE'] = df['CWE'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')
# Add a 'Count' column with all values set to 1 (syntactic sugar to make counts and sums and forecasting easy)
df['Count'] = 1

# Show the last few rows of the DataFrame
print(df.tail)

"""Save all the data we just filtered to a CSV file, for future use."""

df.head()

# Check if the directory for the CSV file exists, and create it if necessary
csv_file_path = 'NVD-Vulnerability-Volumes.csv'

# Check if the file already exists
if os.path.exists(csv_file_path):
    # If the file exists, read it into a DataFrame
    existing_data = pd.read_csv(csv_file_path, index_col='ID')

    # Merge the existing data with the new data
    all_items = pd.concat([existing_data, df.set_index('ID')])

    # Drop duplicate rows based on the 'cve_id' column, keeping the latest entry
    all_items = all_items[~all_items.index.duplicated(keep='last')]

# Reset the index to publication after dedupping based on IDs
all_items = df.set_index('Publication')

# Sort the data by the index (published date)
all_items.sort_index(inplace=True)

# Save the merged data back to the CSV file
all_items.to_csv(csv_file_path)

"""# Now we want to clone this data frame and explode the cpe column so we have a ready made dataframe that can do vendor and product forecasting"""

all_items

def process_cpe_dataframe(df):
    # Explode the 'v2.3 CPE' column to create a new row for each CPE string
    df = df.explode('v2.3 CPE')

    def extract_cpe_parts(cpe_str):
        if pd.isna(cpe_str) or not isinstance(cpe_str, str):
            return pd.Series({
                'Part': None, 'Vendor': None, 'Product': None,
                'Version': None, 'Update': None, 'Edition': None,
                'Language': None, 'SW_Edition': None, 'Target_SW': None,
                'Target_HW': None, 'Other': None
            })

        cpe_str = cpe_str.strip('"')
        parts = cpe_str.split(':')

        # Ensure we have enough parts
        if len(parts) >= 13:
            return pd.Series({
                'Part': parts[2],
                'Vendor': parts[3],
                'Product': parts[4],
                'Version': parts[5],
                'Update': parts[6],
                'Edition': parts[7],
                'Language': parts[8],
                'SW_Edition': parts[9],
                'Target_SW': parts[10],
                'Target_HW': parts[11],
                'Other': parts[12] if len(parts) > 12 else None
            })

        return pd.Series({
            'Part': None, 'Vendor': None, 'Product': None,
            'Version': None, 'Update': None, 'Edition': None,
            'Language': None, 'SW_Edition': None, 'Target_SW': None,
            'Target_HW': None, 'Other': None
        })

    # Apply the extraction function to each row in the DataFrame
    extracted_parts = df['v2.3 CPE'].apply(lambda x: extract_cpe_parts(x))

    # Concatenate the original DataFrame with the extracted parts
    df = pd.concat([df, extracted_parts], axis=1)

    return df

# Re-run the function with the corrected implementation
cpe_df = process_cpe_dataframe(all_items)

cpe_df.head()

# Remove rows where 'v2.3 CPE' column is NaN
cpe_df = cpe_df.dropna(subset=['v2.3 CPE'])

# Remove rows where 'VulnStatus' column is 'Rejected'
cpe_df = cpe_df[cpe_df['VulnStatus'] != 'Rejected']

# Reset the index to make 'Publication' a column
cpe_df.reset_index(inplace=True)

# Set a multi-index with 'ID' and 'v2.3 CPE'
cpe_df.set_index(['ID', 'v2.3 CPE'], inplace=True)

# Check if the file already exists
csv_file_path = 'Vendor-Product-Vulnerability-Volumes.csv'
if os.path.exists(csv_file_path):
    # If the file exists, read it into a DataFrame
    existing_data = pd.read_csv(csv_file_path, low_memory=False)

    # Merge the existing data with the new data on ID and CPE
    merged_cpe_df = pd.concat([existing_data, cpe_df], ignore_index=True)

    # Ensure uniqueness by considering both ID and CPE columns
    merged_cpe_df = merged_cpe_df.drop_duplicates(subset=['ID', 'v2.3 CPE'], keep='last')
else:
    # If the file doesn't exist, use the new data as is
    merged_cpe_df = cpe_df.copy()

# Sort the data by the ID column
merged_cpe_df.sort_values(by='ID', inplace=True)

# Save the merged data back to the CSV file
merged_cpe_df.to_csv(csv_file_path, index=True)

"""If you want to read that file in the future, without fetching all the data again, just uncoment the cell below."""

#all_items = pd.read_csv('NVD-Vulnerability-Volumes.csv',index_col=['published'],parse_dates=['published'], low_memory=False)
#all_items = all_items.sort_index()

"""Now we create a vulnlambda file as well, based of the data we already hold.

<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7acd54e3-f1e9-4bb5-a625-0a781a5b944c' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""