# -*- coding: utf-8 -*-
"""YearlyVuln4Cast-V4Vectors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bmi4y9p96Thz9G9i4ZQnbexPDn4NG74z
"""


from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from scipy import stats
from dateutil.parser import parse
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.ar_model import AutoReg
from hurst import compute_Hc
import os
from datetime import date, datetime, timedelta

# Import Data
df = pd.read_csv('NVD-Vulnerability-Volumes.csv', index_col=['Publication'], low_memory=False)
df.index = pd.to_datetime(df.index, errors='coerce')  # Convert index to datetime, handling errors
df['Count'] = 1

# Ensure 'v2 Vector' and 'v3 Vector' are of type object
df['v2 Vector'] = df['v2 Vector'].astype('object')
df['v3 Vector'] = df['v3 Vector'].astype('object')

df.tail()

# Fixing the issue by ensuring the 'Publication' column is properly parsed as datetime and handling NaT values.
# Drop rows with NaT in the 'Publication' index
hurst_df = df.dropna(subset=['Count'])

# Resample the data to daily frequency and sum the counts
daily_counts = hurst_df['Count'].resample('D').sum()

# Drop NaN values after resampling
daily_counts = daily_counts.dropna()

# Compute the Hurst component
H, c, data = compute_Hc(daily_counts, kind='change', simplified=True)

# Plot
f, ax = plt.subplots()
ax.plot(data[0], c * data[0]**H, color="deepskyblue")
ax.scatter(data[0], data[1], color="purple")
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlabel('Time interval')
ax.set_ylabel('R/S ratio')
ax.grid(True)
plt.show()

print("H={:.4f}, c={:.4f}".format(H, c))

stationary = adfuller(df['Count'].resample('Y').sum().to_list())

stationary

diff_stationary = adfuller(np.diff(df['Count'].resample('Y').sum().to_list(),n=1))

diff_stationary

cutoff = datetime(datetime.now().year-1, 12, 31, 0, 0)
#We'll want to check our progress through the quarter so we do need that as a data set later
check=df[df.index.tz_localize(None)>cutoff]
df=df[df.index.tz_localize(None)<=cutoff]
# Construct the model
mod = sm.tsa.SARIMAX(df['Count'].resample('Y').sum(), order=(1, 2, 1), trend='ct')
# Estimate the parameters
res = mod.fit(method="powell",maxiter=500,full_output=True)
print(res.summary())

# Plot diagnostics
res.plot_diagnostics(figsize=(16, 9))
plt.show()

# Here we construct a more complete results object.
fcast_res1 = res.get_forecast(steps=11)

# Most results are collected in the `summary_frame` attribute.
# Here we specify that we want a confidence level of 95%
print(fcast_res1.summary_frame(alpha=0.05))

today = date.today()
current_datetime = today.strftime("%b-%d-%Y")
#Note this format will overwrite all forecasts produced on the same day
file_name = current_datetime+"-Yearly-vuln4cast.csv"
path = 'YearlyForecasts/'
# Check whether the specified path exists or not
isExist = os.path.exists(path)
if not isExist:

   # Create a new directory because it does not exist
   os.makedirs(path)
   print("The new directory is created!")
fcast_res1.summary_frame(alpha=0.05).to_csv(path+file_name)

#Last year's count
df.resample('YE').Count.sum().iloc[-1]

fig, ax = plt.subplots(figsize=(15, 5))
plt.title(current_datetime+"-Yearly Vulnerability Forecast")

# Plot the data (here we are subsetting it to get a closer look at the forecasts than the history)
df['Count'].resample('YE').sum().loc[str(today.year-5):].plot(ax=ax)

# Construct the forecasts
fcast_10 = res.get_forecast(steps=10).summary_frame(alpha=0.10)
fcast_25 = res.get_forecast(steps=10).summary_frame(alpha=0.25)
fcast_50 = res.get_forecast(steps=10).summary_frame(alpha=0.50)
fcast_90 = res.get_forecast(steps=10).summary_frame(alpha=0.90)

# Add a row to the forecast that shows the current numbers and has equal ci
# This makes the display a little nicer to read (eliminating an interstitial space)
interstitial_date = fcast_10.index[0]-timedelta(days=366)
interstitial_date = interstitial_date.tz_localize(None)

new_row = pd.DataFrame(
    {'mean':df.resample('YE').Count.sum().iloc[-1],
     'mean_se':fcast_10.mean_se.iloc[0],
     'mean_ci_lower':df.resample('YE').Count.sum().iloc[-1],
     'mean_ci_upper':df.resample('YE').Count.sum().iloc[-1]},
    index=[interstitial_date])

fcast_10 = pd.concat([new_row,fcast_10.loc[:]])

new_row = pd.DataFrame(
    {'mean':df.resample('YE').Count.sum().iloc[-1],
     'mean_se':fcast_25.mean_se.iloc[0],
     'mean_ci_lower':df.resample('YE').Count.sum().iloc[-1],
     'mean_ci_upper':df.resample('YE').Count.sum().iloc[-1]},
    index=[interstitial_date])

fcast_25 = pd.concat([new_row,fcast_25.loc[:]])

new_row = pd.DataFrame(
    {'mean':df.resample('YE').Count.sum().iloc[-1],
     'mean_se':fcast_50.mean_se.iloc[0],
     'mean_ci_lower':df.resample('YE').Count.sum().iloc[-1],
     'mean_ci_upper':df.resample('YE').Count.sum().iloc[-1]},
    index=[interstitial_date])


fcast_50 = pd.concat([new_row,fcast_50.loc[:]])

new_row = pd.DataFrame(
    {'mean':df.resample('YE').Count.sum().iloc[-1],
     'mean_se':fcast_90.mean_se.iloc[0],
     'mean_ci_lower':df.resample('YE').Count.sum().iloc[-1],
     'mean_ci_upper':df.resample('YE').Count.sum().iloc[-1]},
    index=[interstitial_date])

fcast_90 = pd.concat([new_row,fcast_90.loc[:]])

fcast_10['mean'].plot(ax=ax, style='k--')
ax.fill_between(fcast_10.index, fcast_10['mean_ci_lower'], fcast_10['mean_ci_upper'], color='cyan', alpha=0.25, label='90% Prediction Interval')
ax.fill_between(fcast_25.index, fcast_25['mean_ci_lower'], fcast_25['mean_ci_upper'], color='blue', alpha=0.30, label='75% Prediction Interval')
ax.fill_between(fcast_50.index, fcast_50['mean_ci_lower'], fcast_50['mean_ci_upper'], color='blue', alpha=0.60, label='50% Prediction Interval')
ax.fill_between(fcast_90.index, fcast_90['mean_ci_lower'], fcast_90['mean_ci_upper'], color='purple', alpha=0.60, label='10% Prediction Interval')
ax.grid(which='both')
image_name = current_datetime+"-Yearly-vuln4cast.png"
plt.savefig(path+image_name)

interstitial_date

#Now check how we're doing so far...
print(fcast_res1.summary_frame(alpha=0.05))
print('The current count of published NVD vulns is: '+str(check['Count'].sum()))
print('The current difference is: '+str(check['Count'].sum()- fcast_res1.summary_frame()['mean'].iloc[0]))

mean_v2_vector_forecast = fcast_res1.summary_frame()['mean'].iloc[0]*df['v2 Vector'].dropna().value_counts(normalize=True)
lowerci_v2_vector_forecast = fcast_res1.summary_frame()['mean_ci_lower'].iloc[0]*df['v2 Vector'].dropna().value_counts(normalize=True)
upperci_v2_vector_forecast = fcast_res1.summary_frame()['mean_ci_upper'].iloc[0]*df['v2 Vector'].dropna().value_counts(normalize=True)

#Assemble them and save to csv

mean_v2_vector_forecast.sum()

upperci_v2_vector_forecast.sum()

lowerci_v2_vector_forecast.sum()

mean_v2_vector_forecast.to_csv(path+current_datetime+'-MeanV2VectorForecast.csv')
upperci_v2_vector_forecast.to_csv(path+current_datetime+'-UpperCIV2VectorForecast.csv')
lowerci_v2_vector_forecast.to_csv(path+current_datetime+'-LowerCIV2VectorForecast.csv')

# The error occurs because there are float values (likely NaN) in the 'v3 Vector' column.
# We need to handle these cases explicitly in the functions.

def remove_cvss_prefix(vector):
    if isinstance(vector, str) and vector.startswith("CVSS:3."):
        return vector[8+1:]
    return vector

def remove_double_quotes(input_string):
    if isinstance(input_string, str):
        return input_string.replace('"', '')
    return input_string

# Apply the updated functions to the 'v3 Vector' column
df['v3 Vector'] = df['v3 Vector'].apply(remove_double_quotes)
df['v3 Vector'] = df['v3 Vector'].apply(remove_cvss_prefix)

mean_v3_vector_forecast = fcast_res1.summary_frame()['mean'].iloc[0]*df['v3 Vector'].dropna().value_counts(normalize=True)
lowerci_v3_vector_forecast = fcast_res1.summary_frame()['mean_ci_lower'].iloc[0]*df['v3 Vector'].dropna().value_counts(normalize=True)
upperci_v3_vector_forecast = fcast_res1.summary_frame()['mean_ci_upper'].iloc[0]*df['v3 Vector'].dropna().value_counts(normalize=True)

mean_v3_vector_forecast

mean_v3_vector_forecast.to_csv(path+current_datetime+'-Yearly-MeanV3VectorForecast.csv')
upperci_v3_vector_forecast.to_csv(path+current_datetime+'-Yearly-UpperCIV3VectorForecast.csv')
lowerci_v3_vector_forecast.to_csv(path+current_datetime+'-Yearly-LowerCIV3VectorForecast.csv')

#This should be improved, from a multiplication to a forecast for each one
simple_assigner_forecast = fcast_res1.summary_frame()['mean'].iloc[0]*df['ASSIGNER'].value_counts(normalize=True)

simple_assigner_forecast

import matplotlib.pyplot as plt

# Filter assigners with values greater than or equal to 100
filtered_assigner_forecast = simple_assigner_forecast[simple_assigner_forecast >= 100]

# Sort the filtered data in descending order
sorted_assigner_forecast = filtered_assigner_forecast.sort_values(ascending=False)

# Plot the horizontal bar chart
plt.figure(figsize=(12, 8))
sorted_assigner_forecast.plot(kind='barh', color='skyblue')
plt.title('Assigner Forecast (Filtered and Sorted)', fontsize=16)
plt.xlabel('Forecasted Count', fontsize=14)
plt.ylabel('Assigner', fontsize=14)
plt.gca().invert_yaxis()  # Invert the y-axis to have the highest at the top
plt.tight_layout()
plt.show()

v3_CVSS_Score_Forecast = fcast_res1.summary_frame()['mean'].iloc[0]*df['v3 CVSS'].value_counts(normalize=True)

check['v3 Vector'] = check['v3 Vector'].apply(remove_double_quotes)
check['v3 Vector'] = check['v3 Vector'].apply(remove_cvss_prefix)
actual = check['v3 Vector'].value_counts()

predicted = mean_v3_vector_forecast

if len(actual) > 0:
    verify_v3_vector = pd.merge(predicted, actual, left_index=True, right_index=True, validate="1:1")
    verify_v3_vector = verify_v3_vector.rename(columns={"v3 Vector_x": "Predicted", "v3 Vector_y": "Actual"})
else:
    verify_v3_vector = predicted

verify_v3_vector.head(20)

verify_v3_vector[verify_v3_vector['count'] > verify_v3_vector.proportion]

"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7acd54e3-f1e9-4bb5-a625-0a781a5b944c' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""